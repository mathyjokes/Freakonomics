---
title: "freakonomics_analysis"
output: html_document
---

First things first - load packages and set up text
```{r}
library(tidyverse)
library(tidytext)
library(readr)
library(rvest)
library(wordcloud)
library(textclean)
library(ggthemes)
library(udpipe)

pods <- read_csv("FREAKONOMICS_EPISODES.csv")

# remember to load up promo_words from next chunk first
# it's, that's, don't, i'm, you're, there's, we're, they're, didn't
pods <- pods %>% 
  mutate(Text = str_replace_all(Text, "Listen and subscribe to our podcast at Apple Podcasts, Stitcher, or elsewhere", "")) %>% 
  mutate(Text = str_replace_all(Text, "Below is a transcript of the episode, edited for readability", "")) %>%
  mutate(Text = str_replace_all(Text, "For more information on the people and ideas in the episode, see the links at the bottom of this post", "")) %>%
  mutate(Text = str_replace_all(Text, "Freakonomics Radio is produced by Stitcher and Dubner Productions", "")) %>%
  mutate(Text = str_replace_all(Text, "This episode was produced by Daphne Chen", "")) %>%
  mutate(Text = str_replace_all(Text, "Our staff also includes Alison Craiglow, Greg Rippin, Matt Hickey, Corinne Wallace, Zack Lapinski, and Mary Diduch", "")) %>%
  mutate(Text = str_replace_all(Text, "Our intern is Emma Tyrrell", "")) %>%
  mutate(Text = str_replace_all(Text, "we had help this week from James Foster", "")) %>%
  mutate(Text = str_replace_all(Text, 'Our theme song is "Mr. Fortune," by the Hitchhikers', "")) %>%
  mutate(Text = str_replace_all(Text, "all the other music was composed by Luis Guerra", "")) %>%
  mutate(Text = str_replace_all(Text, "You can subscribe to Freakonomics Radio on Apple Podcasts, Stitcher, or wherever you get your podcasts", "")) %>%
  
  mutate(Text = tolower(Text)) %>% 
  mutate(Text = replace_contraction(Text)) %>% 
  mutate(Text = str_replace_all(Text, "it's", "it is")) %>% 
  mutate(Text = str_replace_all(Text,  "that's","that is")) %>% 
  mutate(Text = str_replace_all(Text,  "that\'s","that is")) %>% 
  mutate(Text = str_replace_all(Text, "don't", "do not")) %>% 
  mutate(Text = str_replace_all(Text, "i'm", "i am")) %>%
  mutate(Text = str_replace_all(Text, "i\'m", "i am")) %>%
  mutate(Text = str_replace_all(Text, "you're", "you are")) %>% 
  mutate(Text = str_replace_all(Text, "there's", "there is")) %>% 
  mutate(Text = str_replace_all(Text, "we're", "we are")) %>% 
  mutate(Text = str_replace_all(Text, "they're", "they are")) %>% 
  mutate(Text = str_replace_all(Text, "didn't", "did not")) %>% 
  mutate(Text = str_replace_all(Text, "can't", "can not")) %>% 
  mutate(Text = str_replace_all(Text, "i'd", "i would")) %>% 
  mutate(Text = str_replace_all(Text, "what's", "what is")) %>% 
  mutate(Text = str_replace_all(Text, "let's", "let us")) %>% 
  mutate(Text = str_replace_all(Text, "i've", "i have")) %>% 
  mutate(Text = str_replace_all(Text, "we've", "we have")) %>% 
  mutate(Text = str_replace_all(Text, "you've", "you have")) %>% 
  mutate(Text = str_replace_all(Text, "doesn't", "does not")) %>% 
  mutate(Text = str_replace_all(Text, "he's", "he is")) %>% 
  mutate(Text = str_replace_all(Text, "she's", "she is")) %>% 
  mutate(Text = str_replace_all(Text, "you'll", "you will")) %>% 
  mutate(Text = str_replace_all(Text, "here's", "here is")) %>% 
  mutate(Text = str_replace_all(Text, "wasn't", "was not")) %>% 
  mutate(Text = str_replace_all(Text, "is'nt", "is not")) %>% 
  mutate(Text = str_replace_all(Text, "we'll", "we will")) %>% 
  mutate(Text = str_replace_all(Text, "wouldn't", "would not")) %>% 
  mutate(Text = str_replace_all(Text, "here's", "here is"))
```


Using Udpipe!
```{r}
####################### Seperate sentences into their own row #######################
podcast_scripts <- pods %>% 
  mutate(Text = str_replace_all(Text, "\n", "")) %>% 
  separate_rows(Text, sep = "\\.") 

####################### There are 131,067 sentences.... #######################
podcast_scripts %>% 
  glimpse()

####################### Download and load premade udpipe model #######################
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = 'english-ewt-ud-2.4-190531.udpipe')

####################### Run the model over the transcripts #######################
podcast_annotate <- udpipe_annotate(udmodel_english, podcast_scripts$Text)
podcast_annotate_df <- data.frame(podcast_annotate)

####################### There are 2,464,503 tokens.... #######################
podcast_annotate_df %>% 
  glimpse()

####################### And 32,566 unique lemmas #######################
podcast_annotate_df %>% 
  count(lemma) %>% 
  glimpse()


####################### Most common occuring parts of speech #######################

#################### Nouns ####################
# make it a table #
ggsave('freak_common_nouns.png',
podcast_annotate_df %>% 
  filter(upos == "NOUN") %>% 
  count(lemma, sort = T, name = "count") %>% 
  head(20) %>% 
  mutate(lemma = reorder(lemma, count))%>% 
  ggplot(aes(lemma, count)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() + 
    theme_fivethirtyeight() +
    labs(title = "Most Common Nouns")
)

# make it a wordcloud! #
ggsave('freak_wordcloud_noun.png',
podcast_annotate_df %>% 
  filter(upos == "NOUN") %>% 
  count(lemma, sort = T, name = "count") %>% 
  filter(lemma != "dubner") %>% 
  filter(lemma != "freakonomics") %>% 
  filter(lemma != "radio") %>% 
  filter(lemma != "lot") %>% 
  filter(lemma != "episode") %>% 
  filter(lemma != "yeah") %>% 
  filter(lemma != "podcast") %>% 
  filter(lemma != "10") %>% 
  with(wordcloud(lemma, count, max.words = 100))
)

#################### Verbs ####################
# make it a table #
ggsave('freak_common_verbs.png',
podcast_annotate_df %>% 
  filter(upos == "VERB") %>% 
  count(lemma, sort = T, name = "count") %>% 
  head(20) %>% 
  mutate(lemma = reorder(lemma, count))%>% 
  ggplot(aes(lemma, count)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() + 
    theme_fivethirtyeight() +
    labs(title = "Most Common Verbs")
)

# make it a wordcloud! #
ggsave('freak_wordcloud_verb.png',
podcast_annotate_df %>% 
  filter(upos == "VERB") %>% 
  count(lemma, sort = T, name = "count") %>% 
  with(wordcloud(lemma, count, max.words = 100))
)

#################### Nouns ####################
# make it a table #
ggsave('freak_common_adjs.png',
podcast_annotate_df %>% 
  filter(upos == "ADJ") %>% 
  count(lemma, sort = T, name = "count") %>% 
  head(20) %>% 
  mutate(lemma = reorder(lemma, count))%>% 
  ggplot(aes(lemma, count)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() + 
    theme_fivethirtyeight() +
    labs(title = "Most Common Adjectives")
)

# make it a wordcloud! #
ggsave('freak_wordcloud_adj.png',
podcast_annotate_df %>% 
  filter(upos == "ADJ") %>% 
  count(lemma, sort = T, name = "count") %>% 
  with(wordcloud(lemma, count, max.words = 100))
)

####################### Get top noun-adjective pairs via RAKE #######################
noun_adj <- keywords_rake(x = podcast_annotate_df, term = "lemma", group = "doc_id", 
                       relevant = podcast_annotate_df$upos %in% c("NOUN", "ADJ"))


######## Visualize that ########
ggsave('freak_rake.png',
noun_adj %>% 
  filter(ngram >1 2) %>% 
  head(20) %>%
  ggplot(., aes(x = fct_reorder(keyword, rake), y = rake)) +
    geom_histogram(stat = "identity") + 
    coord_flip() + 
    labs(title = "Freakonomics Radio Top Phrases",
         subtitle = "Identified by Rapid Automatic Keyword Extraction (RAKE)",
         y = "RAKE",
         x = "Phrase")
)


####################### Get top noun-verb pairs #######################
podcast_annotate_df$phrase_tag <- as_phrasemachine(podcast_annotate_df$upos, type = "upos")
noun_verb <- keywords_phrases(x = podcast_annotate_df$phrase_tag, term = tolower(podcast_annotate_df$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)


######## Visualize that ########
ggsave('freak_noun_verb.png',
noun_verb %>% 
  filter(ngram > 1) %>% 
  arrange(desc(freq)) %>% 
  head(20) %>%
  ggplot(., aes(x = fct_reorder(keyword, freq), y = freq)) +
    geom_histogram(stat = "identity") + 
    coord_flip() + 
    labs(title = "Freakonomics Radio Top Noun-Verb Phrases",
         y = "Frequency",
         x = "Phrase")
)
```
